{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AirBnB Data Wrangling: Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About the Data\n",
    "\n",
    "The data we are using for this project is "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "import re\n",
    "import glob as glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_review_data(directory):\n",
    "    #Read Reviews\n",
    "    reviews_full = pd.read_csv(directory + 'reviews_full.csv').drop(columns=['Unnamed: 0'])\n",
    "            \n",
    "    return reviews_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select City\n",
    "country = 'united-states'\n",
    "city = 'san-francisco'\n",
    "\n",
    "# Directory\n",
    "directory = '../data/' + country + '/' + city + '/'\n",
    "\n",
    "# Download Review Data\n",
    "reviews_full = load_review_data(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>listing_id</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>reviewer_id</th>\n",
       "      <th>reviewer_name</th>\n",
       "      <th>comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>958</td>\n",
       "      <td>5977</td>\n",
       "      <td>2009-07-23</td>\n",
       "      <td>15695</td>\n",
       "      <td>Edmund C</td>\n",
       "      <td>Our experience was, without a doubt, a five st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>958</td>\n",
       "      <td>6660</td>\n",
       "      <td>2009-08-03</td>\n",
       "      <td>26145</td>\n",
       "      <td>Simon</td>\n",
       "      <td>Returning to San Francisco is a rejuvenating t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>958</td>\n",
       "      <td>11519</td>\n",
       "      <td>2009-09-27</td>\n",
       "      <td>25839</td>\n",
       "      <td>Denis</td>\n",
       "      <td>We were very pleased with the accommodations a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>958</td>\n",
       "      <td>16282</td>\n",
       "      <td>2009-11-05</td>\n",
       "      <td>33750</td>\n",
       "      <td>Anna</td>\n",
       "      <td>We highly recommend this accomodation and agre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>958</td>\n",
       "      <td>26008</td>\n",
       "      <td>2010-02-13</td>\n",
       "      <td>15416</td>\n",
       "      <td>Venetia</td>\n",
       "      <td>Holly's place was great. It was exactly what I...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   listing_id     id        date  reviewer_id reviewer_name  \\\n",
       "0         958   5977  2009-07-23        15695      Edmund C   \n",
       "1         958   6660  2009-08-03        26145         Simon   \n",
       "2         958  11519  2009-09-27        25839         Denis   \n",
       "3         958  16282  2009-11-05        33750          Anna   \n",
       "4         958  26008  2010-02-13        15416       Venetia   \n",
       "\n",
       "                                            comments  \n",
       "0  Our experience was, without a doubt, a five st...  \n",
       "1  Returning to San Francisco is a rejuvenating t...  \n",
       "2  We were very pleased with the accommodations a...  \n",
       "3  We highly recommend this accomodation and agre...  \n",
       "4  Holly's place was great. It was exactly what I...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 311277 entries, 0 to 311276\n",
      "Data columns (total 6 columns):\n",
      "listing_id       311277 non-null int64\n",
      "id               311277 non-null int64\n",
      "date             311277 non-null object\n",
      "reviewer_id      311277 non-null int64\n",
      "reviewer_name    311276 non-null object\n",
      "comments         311198 non-null object\n",
      "dtypes: int64(3), object(3)\n",
      "memory usage: 14.2+ MB\n"
     ]
    }
   ],
   "source": [
    "reviews_full.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Null Comments\n",
    "reviews_full = reviews_full.dropna(subset=['comments'])\n",
    "\n",
    "# Replace html \n",
    "reviews_full['comments'] = reviews_full['comments'].str.replace('\\n',' ')\n",
    "\n",
    "# Remove all ascii\n",
    "reviews_full['comments'] = reviews_full['comments'].map(lambda x: re.sub(r'[^\\x00-\\x7F]+',' ', x))\n",
    "\n",
    "# Dropna\n",
    "reviews_full = reviews_full.dropna(subset=['comments'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Stopwords & Lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "import re\n",
    "\n",
    "# Initialize Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Get Stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Get Puncuations\n",
    "punctuations = set(string.punctuation)\n",
    "\n",
    "def clean_text(doc):\n",
    "\n",
    "    # Tokenize, Lemmatize, and Remove Stopwords\n",
    "    tokens = [lemmatizer.lemmatize(word).lower() for word in nltk.word_tokenize(doc) if (word.lower() not in set(stop_words | punctuations) and (len(word) > 2))]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean & Tokenize Overviews\n",
    "reviews_full.loc[:,'tokens'] = reviews_full['comments'].apply(clean_text)\n",
    "\n",
    "# Count Tokens\n",
    "reviews_full['tokens_count'] = reviews_full['tokens'].map(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.figure(figsize=(8,4))\n",
    "_ = reviews_full['tokens_count'].hist(bins=100)\n",
    "np.mean(reviews_full['tokens_count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos_sentences(data):\n",
    "    # Tokenize each sentence into words: token_sentences\n",
    "    token_sentences = [nltk.word_tokenize(re.sub(r'[^\\x00-\\x7F]+',' ', sent)) for sent in data]\n",
    "\n",
    "    # Tag each tokenized sentence into parts of speech: pos_sentences\n",
    "    pos_sentences = [nltk.pos_tag(sent) for sent in token_sentences]\n",
    "    return pos_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize TextExtraction with DS Skills Listing\n",
    "pos_overviews_reviews = get_pos_sentences(reviews_full['comments'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_name_entities(pos_sentences):\n",
    "    \"\"\"Return Name Entities List\"\"\"\n",
    "    # Create the named entity chunks: chunked_sentences\n",
    "    chunked_sentences = nltk.ne_chunk_sents(pos_sentences, binary=True)\n",
    "        \n",
    "    # Chunk List per Listing\n",
    "    listing_chunk_list = []\n",
    "\n",
    "    # Test for stems of the tree with 'NE' tags\n",
    "    for sent in chunked_sentences:\n",
    "        chunks_list = []\n",
    "        for chunk in sent:\n",
    "            if hasattr(chunk, \"label\") and chunk.label() == \"NE\":\n",
    "                chunks_list.append(chunk.leaves())\n",
    "\n",
    "        #Append List of Chunks\n",
    "        listing_chunk_list.append(chunks_list)\n",
    "        \n",
    "    # Chunks to List of Words\n",
    "    listing_name_entity_list = [[' '.join([word[0] for word in chunk]) for chunk in chunk_list] for chunk_list in listing_chunk_list]\n",
    "                \n",
    "    return listing_name_entity_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Name Entities\n",
    "ner_overviews_reviews = get_name_entities(pos_overviews_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset Index\n",
    "reviews_full = reviews_full.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Name Entities to Df\n",
    "reviews_full['name_entities'] = pd.Series(ner_overviews_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count Name Entities\n",
    "reviews_full['name_entities_count'] = reviews_full['name_entities'].map(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_full.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete Name Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_name_entities(row):\n",
    "    name_entities = row.name_entities\n",
    "    comments_w_ne = row.comments\n",
    "    \n",
    "    for ne in name_entities:\n",
    "        comments_w_ne = comments_w_ne.replace(ne, '')\n",
    "        \n",
    "    return comments_w_ne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "reviews_full['comments_no_ne'] = reviews_full.apply(del_name_entities, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count No NER Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean & Tokenize Overviews\n",
    "reviews_full['no_ne_tokens'] = reviews_full['comments_no_ne'].apply(clean_text)\n",
    "\n",
    "# Count Tokens\n",
    "reviews_full['no_ne_tokens_count'] = reviews_full['no_ne_tokens'].map(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_full['no_ne_tokens_count'].hist(bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nouns(pos_sentences):\n",
    "    \"\"\"Return Noun List\"\"\"\n",
    "    # Noun Codes\n",
    "    noun_code = ['NN','NNS','NNP','NNPS']\n",
    "    \n",
    "    # Get Nouns\n",
    "    noun_list = [[word[0].lower() for word in sent if word[1] in noun_code] for sent in pos_sentences]\n",
    "\n",
    "    #Lemmatize\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    noun_lem_list = [[lemmatizer.lemmatize(noun) for noun in noun_sent] for noun_sent in noun_list]\n",
    "        \n",
    "    return noun_lem_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Nouns to DF\n",
    "reviews_full['nouns'] = get_nouns(pos_overviews_reviews)\n",
    "\n",
    "# Count Nouns Per Review\n",
    "reviews_full['nouns_counts'] = reviews_full['nouns'].map(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_verbs(pos_sentences):\n",
    "    \"\"\"Return Verbs\"\"\"\n",
    "    # Codes\n",
    "    verb_code = ['VBG','VB','VBD','VBN','VBZ']\n",
    "\n",
    "    # Get List of Adjectives\n",
    "    verb_list = [[word[0].lower() for word in sent if word[1] in verb_code] for sent in pos_sentences]\n",
    "\n",
    "    # Lemmatize\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    verb_lem_list = [[lemmatizer.lemmatize(verb, 'v') for verb in verb_sent] for verb_sent in verb_list]\n",
    "        \n",
    "    return verb_lem_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Verbs Tokens to DF\n",
    "reviews_full['verbs'] = get_verbs(pos_overviews_reviews)\n",
    "\n",
    "# Count Verbs Per Review\n",
    "reviews_full['verbs_counts'] = reviews_full['verbs'].map(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Adjectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_adjectives(pos_sentences):\n",
    "    \"\"\"Adjective List\"\"\"\n",
    "    # Codes\n",
    "    adj_code = ['JJ','JJR','JJS']\n",
    "\n",
    "    # Get List of Adjectives\n",
    "    adj_list = [[word[0].lower() for word in sent if word[1] in adj_code] for sent in pos_sentences]\n",
    "\n",
    "    # Lemmatize\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    adj_lem_list = [[lemmatizer.lemmatize(adj, 'a') for adj in adj_sent] for adj_sent in adj_list]\n",
    "        \n",
    "    return adj_lem_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Adjectives to DF\n",
    "reviews_full['adjectives'] = get_adjectives(pos_overviews_reviews)\n",
    "\n",
    "# Count Adjectives Per Review\n",
    "reviews_full['adjectives_counts'] = reviews_full['adjectives'].map(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Organize Columns\n",
    "review_cols = ['listing_id','id','date','comments','tokens','tokens_count','name_entities','name_entities_count','comments_no_ne','no_ne_tokens','no_ne_tokens_count','nouns','nouns_counts','verbs','verbs_counts','adjectives','adjectives_counts']\n",
    "reviews_full = reviews_full[review_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_full.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export To CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(directory + 'interim/'):\n",
    "    os.makedirs(directory + 'interim/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_full.to_csv(directory + 'interim/review_wrangled.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
