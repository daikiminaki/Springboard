{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns; sns.set()\n",
    "import glob as glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sample_data(country, city):\n",
    "    directory = 'data/' + country + '/' + city + '/'\n",
    "    csv_files = [file_name for file_name in glob.glob(directory + '*') if file_name[-4:] == '.csv']\n",
    "    \n",
    "    for file_loc in csv_files:\n",
    "        file_name = file_loc.split('/')[3]\n",
    "        \n",
    "        if file_name == 'listings_sample.csv':\n",
    "            #Read Listing Sample\n",
    "            listing_sample = pd.read_csv(file_loc)\n",
    "            \n",
    "        elif file_name == 'reviews_sample.csv':\n",
    "            #Read Review Sample\n",
    "            reviews_sample = pd.read_csv(file_loc)\n",
    "            \n",
    "        elif file_name == 'neighbourhoods_sample.csv':\n",
    "            #Read Neighborhoods\n",
    "            neighbourhoods_sample = pd.read_csv(file_loc)\n",
    "            \n",
    "    return [listing_sample, reviews_sample, neighbourhoods_sample]\n",
    "\n",
    "\n",
    "def load_full_data(country, city):\n",
    "    directory = 'data/' + country + '/' + city + '/'\n",
    "    csv_files = [file_name for file_name in glob.glob(directory + '*') if file_name[-4:] == '.csv']\n",
    "    \n",
    "    for file_loc in csv_files:\n",
    "        file_name = file_loc.split('/')[3]\n",
    "        \n",
    "        if file_name == 'listings_full.csv':\n",
    "            #Read Listings\n",
    "            listings_full = pd.read_csv(file_loc).drop(columns=['Unnamed: 0'])\n",
    "            \n",
    "        elif file_name == 'reviews_full.csv':\n",
    "            #Read Reviews\n",
    "            reviews_full = pd.read_csv(file_loc).drop(columns=['Unnamed: 0'])\n",
    "            \n",
    "        elif file_name == 'calendar_full.csv':\n",
    "            #Read Calendar\n",
    "            calendar_full = pd.read_csv(file_loc).drop(columns=['Unnamed: 0'])\n",
    "            \n",
    "    return [listings_full, reviews_full, calendar_full]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "listings_sf = pd.read_csv('../sf/listings_detailed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_listing_full(listing_full_data):\n",
    "    \"\"\"Cleans listing_full.csv data\"\"\"\n",
    "    # Input Data\n",
    "    df = listing_full_data\n",
    "    \n",
    "    # String to Datetime\n",
    "    df['last_scraped'] = pd.to_datetime(df['last_scraped'])\n",
    "    df['host_since'] = pd.to_datetime(df['host_since'])\n",
    "    df['calendar_last_scraped'] = pd.to_datetime(df['calendar_last_scraped'])\n",
    "    df['first_review'] = pd.to_datetime(df['first_review'])\n",
    "    df['last_review'] = pd.to_datetime(df['last_review'])\n",
    "    \n",
    "    # String to Numeric\n",
    "    df['host_response_rate'] = pd.to_numeric(df['host_response_rate'].str[:-1]) / 100\n",
    "    df['price'] = pd.to_numeric(df['price'].str[1:].str.replace(',',''))\n",
    "    df['weekly_price'] = pd.to_numeric(df['weekly_price'].str[1:].str.replace(',',''))\n",
    "    df['monthly_price'] = pd.to_numeric(df['monthly_price'].str[1:].str.replace(',',''))\n",
    "    df['security_deposit'] = pd.to_numeric(df['security_deposit'].str[1:].str.replace(',',''))\n",
    "    df['cleaning_fee'] = pd.to_numeric(df['cleaning_fee'].str[1:].str.replace(',',''))\n",
    "    df['extra_people'] = pd.to_numeric(df['extra_people'].str[1:].str.replace(',',''))\n",
    "\n",
    "    # t/f to Numeric\n",
    "    df['host_is_superhost'] = (df['host_is_superhost'] == \"t\").astype(int)\n",
    "    df['host_has_profile_pic'] = (df['host_has_profile_pic'] == \"t\").astype(int)\n",
    "    df['host_identity_verified'] = (df['host_identity_verified'] == \"t\").astype(int)\n",
    "    df['is_location_exact'] = (df['is_location_exact'] == \"t\").astype(int)\n",
    "    df['has_availability'] = (df['has_availability'] == \"t\").astype(int)\n",
    "    df['requires_license'] = (df['requires_license'] == \"t\").astype(int)\n",
    "    df['instant_bookable'] = (df['instant_bookable'] == \"t\").astype(int)\n",
    "    df['is_business_travel_ready'] = (df['is_business_travel_ready'] == \"t\").astype(int)\n",
    "    df['require_guest_profile_picture'] = (df['require_guest_profile_picture'] == \"t\").astype(int)\n",
    "    df['require_guest_phone_verification'] = (df['require_guest_phone_verification'] == \"t\").astype(int)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_listings_sf = clean_listing_full(listings_sf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "listings_detailed = clean_listings_sf.copy()\n",
    "\n",
    "ID = list(listings_detailed.iloc[:,:2].columns)\n",
    "\n",
    "ABOUT_COLS = list(listings_detailed.iloc[:,3:15].columns)\n",
    "\n",
    "PICS_COLS = list(listings_detailed.iloc[:,15:19].columns)\n",
    "\n",
    "HOST_COLS = list(listings_detailed.columns[listings_detailed.columns.str.contains('host')])\n",
    "\n",
    "NEIGHBORHOOD_COLS = list(listings_detailed.columns[listings_detailed.columns.str.contains('neighbourhood')])\n",
    "\n",
    "LOCATION_COLS = list(listings_detailed.iloc[:,37:51].columns)\n",
    "\n",
    "PROPERTY_COLS = list(listings_detailed.iloc[:,51:60].columns)\n",
    "\n",
    "PRICE_COLS = list(listings_detailed.iloc[:,60:67].columns)\n",
    "\n",
    "NIGHTS_COLS = list(listings_detailed.columns[listings_detailed.columns.str.contains('mum')])\n",
    "\n",
    "AVAILABILITY_COLS = list(listings_detailed.columns[listings_detailed.columns.str.contains('availability')])\n",
    "\n",
    "REVIEW_COLS = list(listings_detailed.columns[listings_detailed.columns.str.contains('review')])\n",
    "\n",
    "SCRAPING_COLS = ['scrape_id','calendar_updated','calendar_last_scraped']\n",
    "\n",
    "ELSE_COLS = ['requires_license', 'license', 'jurisdiction_names', 'instant_bookable',\\\n",
    "             'is_business_travel_ready', 'cancellation_policy', 'require_guest_profile_picture',\\\n",
    "             'require_guest_phone_verification']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neighborhood Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighborhood_info = clean_listings_sf[ID + NEIGHBORHOOD_COLS + ABOUT_COLS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7198 entries, 0 to 7197\n",
      "Data columns (total 18 columns):\n",
      "id                              7198 non-null int64\n",
      "listing_url                     7198 non-null object\n",
      "host_neighbourhood              6559 non-null object\n",
      "neighbourhood                   6660 non-null object\n",
      "neighbourhood_cleansed          7198 non-null object\n",
      "neighbourhood_group_cleansed    0 non-null float64\n",
      "last_scraped                    7198 non-null datetime64[ns]\n",
      "name                            7198 non-null object\n",
      "summary                         7000 non-null object\n",
      "space                           6109 non-null object\n",
      "description                     7183 non-null object\n",
      "experiences_offered             7198 non-null object\n",
      "neighborhood_overview           5310 non-null object\n",
      "notes                           4486 non-null object\n",
      "transit                         5238 non-null object\n",
      "access                          4794 non-null object\n",
      "interaction                     4931 non-null object\n",
      "house_rules                     5307 non-null object\n",
      "dtypes: datetime64[ns](1), float64(1), int64(1), object(15)\n",
      "memory usage: 1012.3+ KB\n"
     ]
    }
   ],
   "source": [
    "neighborhood_info.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighborhood_overviews = neighborhood_info[['id','neighbourhood_cleansed','neighborhood_overview']].dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean & Tokenize Overview Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "import re\n",
    "\n",
    "# Initialize Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Get Stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Get Puncuations\n",
    "punctuations = set(string.punctuation)\n",
    "\n",
    "def clean_text(doc):\n",
    "    \n",
    "    # remove all ascii\n",
    "    doc = re.sub(r'[^\\x00-\\x7F]+',' ', doc)\n",
    "\n",
    "    # Tokenize, Lemmatize, and Remove Stopwords\n",
    "    tokens = [lemmatizer.lemmatize(word).lower() for word in nltk.word_tokenize(doc) if word.lower() not in set(stop_words | punctuations)]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean & Tokenize Overviews\n",
    "neighborhood_overviews['tokens'] = neighborhood_overviews['neighborhood_overview'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighborhood_overviews['clean_overviews'] = neighborhood_overviews['tokens'].map(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos_sentences(data):\n",
    "    # Tokenize each sentence into words: token_sentences\n",
    "    token_sentences = [nltk.word_tokenize(re.sub(r'[^\\x00-\\x7F]+',' ', sent)) for sent in data]\n",
    "\n",
    "    # Tag each tokenized sentence into parts of speech: pos_sentences\n",
    "    pos_sentences = [nltk.pos_tag(sent) for sent in token_sentences]\n",
    "    return pos_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize TextExtraction with DS Skills Listing\n",
    "pos_overviews_neighborhood = get_pos_sentences(neighborhood_overviews['neighborhood_overview'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_non_name_entities(pos_sentences):\n",
    "    \"\"\"Return Name Entities List\"\"\"\n",
    "    # Create the named entity chunks: chunked_sentences\n",
    "    chunked_sentences = nltk.ne_chunk_sents(pos_sentences, binary=True)\n",
    "        \n",
    "    # Chunk List per Listing\n",
    "    listing_chunk_list = []\n",
    "\n",
    "    # Test for stems of the tree with 'NE' tags\n",
    "    for sent in chunked_sentences:\n",
    "        chunks_list = []\n",
    "        for chunk in sent:\n",
    "            if hasattr(chunk, \"label\") and chunk.label() == \"NE\":\n",
    "                chunks_list.append(chunk.leaves())\n",
    "\n",
    "        #Append List of Chunks\n",
    "        listing_chunk_list.append(chunks_list)\n",
    "        \n",
    "    # Chunks to List of Words\n",
    "    listing_name_entity_list = [[' '.join([word[0] for word in chunk]) for chunk in chunk_list] for chunk_list notin listing_chunk_list]\n",
    "                \n",
    "    return listing_name_entity_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_texts = list(neighborhood_overviews['tokens'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "# Create a corpus from a list of texts\n",
    "common_dictionary = Dictionary(token_texts)\n",
    "common_corpus = [common_dictionary.doc2bow(text) for text in token_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "import time\n",
    "ldam = LdaMulticore\n",
    "\n",
    "num_topics = 50\n",
    "num_words = 10\n",
    "passes = 50\n",
    "\n",
    "# Get Start Time\n",
    "start_time = time.time()\n",
    "\n",
    "# LDA Model\n",
    "ldam_model = ldam(common_corpus, num_topics=num_topics, id2word=common_dictionary, passes=passes)\n",
    "model_end_time = time.time() # Model End Time\n",
    "\n",
    "# LDA Results\n",
    "results = ldam_model.print_topics(num_topics=num_topics, num_words=num_words)\n",
    "result_time = time.time() # Results Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_results(results):\n",
    "    for index, results in results:\n",
    "        print(str(index) + ': ' + str(', '.join(results.split('\"')[1::2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: min, wharf, walk, fisherman, 's, minute, 5, 10, square, 15\n",
      "1: mi, sunset, day, time, watch, outer, street, world, 's, francisco\n",
      "2: street, block, restaurant, away, san, francisco, marina, bridge, gate, golden\n",
      "3: bay, san, francisco, location, mission, 4th, caltrain, street, south, neighborhood\n",
      "4: bar, door, cafe, shop, district, mission, francisco, haight, san, neighborhood\n",
      "5: neighborhood, valley, noe, family, quiet, francisco, san, safe, friendly, city\n",
      "6: parking, street, free, car, francisco, san, neighborhood, one, park, please\n",
      "7: residential, mostly, area, supermarket, whole, foods, neighborhood, shop, restaurant, new\n",
      "8: court, tennis, min, playground, walk, park, dog, basketball, center, coffee\n",
      "9: neighborhood, ``, san, '', block, francisco, one, n't, city, place\n",
      "10: francisco, san, city, 's, one, hill, distance, shopping, building, access\n",
      "11: located, neighborhood, laurel, sacramento, district, heights, centrally, presidio, village, good\n",
      "12: building, neighborhood, tenderloin, car, cable, favorite, hill, close, great, include\n",
      "13: neighborhood, minute, san, francisco, west, downtown, area, portal, restaurant, public\n",
      "14: store, restaurant, grocery, shop, neighborhood, block, coffee, away, bar, market\n",
      "15: close, san, francisco, 101, restaurant, 280, neighborhood, highway, access, easy\n",
      "16: neighborhood, san, francisco, city, one, 's, restaurant, downtown, bar, street\n",
      "17: block, away, walk, restaurant, great, shop, one, park, place, coffee\n",
      "18: chinatown, north, little, great, beach, wharf, city, walk, italy, fisherman\n",
      "19: haight, park, valley, neighborhood, castro, block, cole, located, gate, golden\n",
      "20: park, neighborhood, haight, garden, one, san, francisco, haight-ashbury, love, summer\n",
      "21: great, restaurant, square, neighborhood, park, nopa, alamo, divisadero, view, food\n",
      "22: noriega, block, beach, cafe, market, produce, away, coffee, corridor, restaurant\n",
      "23: minute, walk, restaurant, street, valley, mission, noe, park, away, neighborhood\n",
      "24: neighborhood, restaurant, walk, easy, 's, san, francisco, shop, city, world\n",
      "25: mission, block, restaurant, 's, bar, coffee, park, street, valencia, dolores\n",
      "26: everything, need, like, could, location, want, stay, ask, far, find\n",
      "27: square, union, walking, distance, located, district, downtown, restaurant, block, hill\n",
      "28: gate, golden, park, beach, ocean, museum, block, restaurant, neighborhood, garden\n",
      "29: hall, opera, center, symphony, night, hayes, art, sf, civic, restaurant\n",
      "30: hill, bernal, view, neighborhood, city, heights, mission, potrero, restaurant, 's\n",
      "31: neighborhood, hayes, valley, san, park, street, francisco, restaurant, 's, coffee\n",
      "32: 's, neighborhood, one, street, city, ``, people, san, best, beach\n",
      "33: 's, neighborhood, restaurant, block, place, san, http, visitor, cafe, francisco\n",
      "34: san, francisco, shopping, restaurant, home, valley, hayes, street, transportation, community\n",
      "35: --, 's, francisco, san, street, city, urban, park, explore, district\n",
      "36: airbnb, hidden, restaurant, polk, neighborhood, website, located, tech, great, shop\n",
      "37: 's, san, francisco, city, head, center, market, offer, shopping, dining\n",
      "38: mission, bar, san, francisco, neighborhood, restaurant, like, area, find, property\n",
      "39: castro, dolores, mission, park, block, gay, restaurant, shop, bar, historic\n",
      "40: park, mission, nice, neighborhood, terrace, walk, beautiful, tree, store, san\n",
      "41: best, tower, hidden, street, neighborhood, francisco, full, san, north, view\n",
      "42: san, francisco, neighborhood, soma, museum, home, bar, art, street, coffee\n",
      "43: neighborhood, 's, walk, san, francisco, minute, one, best, bernal, heights\n",
      "44: park, glen, canyon, city, restaurant, trail, store, village, neighborhood, hiking\n",
      "45: heights, pacific, area, neighborhood, park, view, fillmore, street, 's, beautiful\n",
      "46: mission, neighborhood, san, francisco, city, street, restaurant, district, away, 's\n",
      "47: block, st, 2, mission, coffee, hotel, street, corner, away, restaurant\n",
      "48: food, chinese, thai, italian, 's, american, restaurant, mexican, japanese, vietnamese\n",
      "49: san, francisco, 's, take, neighborhood, station, district, city, marina, away\n"
     ]
    }
   ],
   "source": [
    "display_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_by_neighborhood = neighborhood_overviews.groupby('neighbourhood_cleansed')['tokens'].apply(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
